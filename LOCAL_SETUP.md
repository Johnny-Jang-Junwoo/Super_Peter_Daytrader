# Local AI Training Setup

## Overview

Your project is now split into two parts:

1. **â˜ï¸ Azure Cloud App** - Lightweight "mailbox" for file uploads (no ML libraries)
2. **ğŸ’» Local Machine** - Heavy AI training with full ML capabilities

## Architecture

```
Friend's Computer              Azure Cloud (Free Tier)          Your Local Machine
    ğŸ“Š CSV Files    â†’    â˜ï¸ Blob Storage (Mailbox)    â†’    ğŸ¤– AI Training
                         (lightweight uploader)            (heavy ML libraries)
```

## Setup Instructions

### 1. Install Local Dependencies

On your **local machine**, install the heavy ML libraries:

```bash
pip install -r requirements-local.txt
```

**Note:** Do NOT install these on Azure - they will crash the Free Tier!

### 2. Configure Environment Variables

Create a `.env` file in the project root:

```bash
# Copy the template
cp .env.example .env

# Edit .env and add your Azure Storage connection string
# Get this from: Azure Portal â†’ Storage Account â†’ Access Keys
```

Your `.env` should look like:
```
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=yourname;AccountKey=yourkey;EndpointSuffix=core.windows.net
```

### 3. Fetch Data from Cloud

Download CSV files uploaded to Azure:

```bash
python fetch_data.py
```

**Output:**
```
ğŸ”Œ Connecting to Cloud Mailbox (trade-uploads)...
ğŸ“¥ Downloading: 2024-12-30_Orders.csv
âœ… Downloaded 1 new file(s) to 'data_pipeline/incoming'
```

Files are saved to: `data_pipeline/incoming/`

### 4. Train AI Model

Run the training pipeline:

```bash
python train_local.py
```

**What it does:**
1. âœ… Loads CSV files from inbox
2. âœ… Fetches market data from yfinance
3. âœ… Merges trades with market candles
4. âœ… Adds technical indicators (RSI, EMA, etc.)
5. âœ… Trains Random Forest model
6. âœ… Saves model to `models/`
7. âœ… Archives processed files

**Output:**
```
ğŸ¤– SUPER PETER LOCAL AI TRAINER
========================================
ğŸ“ Found 1 CSV file(s) to process
âœ… Total trades combined: 28
ğŸ“Š Retrieved 345 market candles
âœ… Training set ready: 331 samples
âœ… Model saved to: models/behavioral_cloner_MNQ_20241230_143022.pkl
ğŸ‰ TRAINING COMPLETE!
```

## Workflow

### Daily Routine

1. **Friend uploads CSV** â†’ Azure Cloud App (â˜ï¸ Mailbox)
2. **You fetch data** â†’ Run `python fetch_data.py`
3. **You train model** â†’ Run `python train_local.py`
4. **Model ready** â†’ Use for predictions locally

### File Locations

```
data_pipeline/
â”œâ”€â”€ incoming/        â† Downloaded CSV files (fresh from cloud)
â””â”€â”€ processed/       â† Archived files after training

models/
â””â”€â”€ behavioral_cloner_*.pkl   â† Trained AI models
```

## Scripts Reference

### `fetch_data.py`
**Purpose:** Download CSV files from Azure Blob Storage

**Usage:**
```bash
python fetch_data.py
```

**Features:**
- âœ… Downloads new files only (skips existing)
- âœ… Shows download progress
- âœ… Creates local inbox directory automatically
- âœ… Lists local files before/after

### `train_local.py`
**Purpose:** Train AI model on downloaded data

**Usage:**
```bash
python train_local.py
```

**Features:**
- âœ… Processes all CSV files in inbox
- âœ… Fetches market data (1-minute OHLCV)
- âœ… Adds technical indicators
- âœ… Trains Random Forest classifier
- âœ… Saves model with timestamp
- âœ… Archives processed files

### `.env.example`
**Purpose:** Template for environment variables

**Setup:**
```bash
cp .env.example .env
# Edit .env with your actual Azure connection string
```

## Troubleshooting

### Issue: "AZURE_STORAGE_CONNECTION_STRING not found"

**Solution:**
1. Create `.env` file: `cp .env.example .env`
2. Get connection string from Azure Portal:
   - Storage Account â†’ Access Keys â†’ Connection String
3. Paste into `.env` file

### Issue: "No CSV files found in data_pipeline/incoming"

**Solution:**
1. Run `python fetch_data.py` first to download files
2. Check Azure Blob Storage has files uploaded
3. Verify container name is "trade-uploads"

### Issue: "No market data available from yfinance"

**Cause:** 1-minute data only available for last 7-30 days

**Solutions:**
- Use recent trade dates (within last week)
- Script will create synthetic data for demonstration
- For historical data, modify to use daily interval

### Issue: "Training failed - insufficient data"

**Solution:**
- Need at least 10 samples with some buy/sell signals
- Upload more CSV files
- Check trades are being matched to market candles

## Advanced Usage

### Batch Processing

Process multiple days of data:

```bash
# Fetch all new files
python fetch_data.py

# Train on everything
python train_local.py
```

### Custom Configuration

Edit `train_local.py` to customize:
- Model parameters (n_estimators, max_depth)
- Feature engineering (add custom indicators)
- Data processing (different intervals)

### Using Trained Models

```python
from trading_bot import BehavioralCloner

# Load trained model
cloner = BehavioralCloner()
cloner.load_brain("models/behavioral_cloner_MNQ_20241230.pkl")

# Make predictions
predictions = cloner.predict(X)
```

## Security Notes

### âš ï¸ Important: .env File

- **NEVER commit `.env` to git** (already in `.gitignore`)
- Contains sensitive Azure credentials
- Each developer needs their own `.env`

### Azure Connection String

- Keep it secret (like a password)
- Rotate periodically in Azure Portal
- Don't share in chat/email

## Performance

### Local vs Cloud

| Feature | Azure Cloud | Local Machine |
|---------|-------------|---------------|
| Purpose | File upload mailbox | AI training |
| RAM Usage | ~100 MB | 1-4 GB |
| CPU Usage | Minimal | Heavy |
| Libraries | 2 (streamlit, azure-storage-blob) | 15+ (includes scikit-learn) |
| Cost | Free Tier | Your electricity ğŸ˜Š |

## Next Steps

1. âœ… Install local dependencies: `pip install -r requirements-local.txt`
2. âœ… Create `.env` file with Azure credentials
3. âœ… Run `python fetch_data.py` to download files
4. âœ… Run `python train_local.py` to train models
5. âœ… Check `models/` folder for trained models
6. ğŸ¯ Use models for predictions!

---

**Questions?** Check the main `README.md` or documentation in `docs/` folder.
